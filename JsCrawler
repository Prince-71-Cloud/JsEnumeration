#!/bin/bash
# ╔══════════════════════════════════════════════════════════╗
# ║           BULLETPROOF JS CRAWLER (Dec 2025)              ║
# ║                                                          ║
# ║               ~5–15 min on 5k+ domains                   ║
# ╚══════════════════════════════════════════════════════════╝
set -euo pipefail

RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'; NC='\033[0m'
FILE="${1:-}"
BASE_PATH="${2:-.}"
TARGET=$(basename "$FILE" | sed 's/\.subdomains\.txt$//' | sed 's/\.txt$//')
OUT="$BASE_PATH/${TARGET}_JS_$(date +%H%M%S)"
THREADS=100
PARALLEL=20  # Number of parallel jobs for gau-tool/wayback

[[ -z "$FILE" || ! -f "$FILE" ]] && { echo -e "${RED}Usage: $0 subdomains.txt [base_output_path]${NC}"; exit 1; }

mkdir -p "$OUT"
echo -e "${GREEN}[+] Bulletproof JS crawl starting → $OUT${NC}\n"

# 1. Get alive 200-OK domains (httpx – bulletproof)
echo -e "${YELLOW}[1] Probing 200-OK domains...${NC}"
cat "$FILE" | sed 's/^/https:\/\//' | \
    httpx -threads "$THREADS" -mc 200 -timeout 7 -retries 0 -fr -o "$OUT/alive.txt"

ALIVE=$(wc -l < "$OUT/alive.txt")
echo -e "${GREEN}[+] $ALIVE alive domains${NC}\n"

# 2. Archive URLs with waybackurls (increased timeout for large domains)
echo -e "${YELLOW}[2] Fetching wayback archive URLs...${NC}"
> "$OUT/archive_urls.txt"
while IFS= read -r domain; do
    echo "  [wayback] $domain"
    # Increased to 120 seconds for domains with large archives
    timeout 120 waybackurls "$domain" 2>/dev/null >> "$OUT/archive_urls.txt" || echo "  [timeout/skip] $domain"
done < "$OUT/alive.txt"
sort -u "$OUT/archive_urls.txt" -o "$OUT/archive_urls.txt" 2>/dev/null || true

echo -e "${GREEN}[+] Archive URLs: $(wc -l < "$OUT/archive_urls.txt")${NC}\n"

# 3. Katana (correct 2025 flags - removed headless mode to avoid timeouts)
echo -e "${YELLOW}[3] Katana crawling...${NC}"
katana -list "$OUT/alive.txt" \
    -d 3 \
    -c "$THREADS" \
    -jc \
    -xhr \
    -timeout 10 \
    -ef png,jpg,jpeg,gif,css,woff,woff2,svg,ico,pdf \
    -o "$OUT/katana_urls.txt" 2>/dev/null
echo -e "${GREEN}[+] Katana URLs: $(wc -l < "$OUT/katana_urls.txt")${NC}"

# 4. Hakrawler (FIXED: no -plain, use stdin pipe)
echo -e "${YELLOW}[4] Hakrawler crawling...${NC}"
cat "$OUT/alive.txt" | while read domain; do
    echo "  [crawling] $domain"
    echo "$domain"
done | hakrawler -d 3 -t 80 -subs -u 2>/dev/null > "$OUT/hakrawler_urls.txt"
echo -e "${GREEN}[+] Hakrawler URLs: $(wc -l < "$OUT/hakrawler_urls.txt")${NC}"

# 5. Extract JS URLs (regex fixed for extensions/queries)
echo -e "${YELLOW}[5] Extracting JS files...${NC}"
cat "$OUT"/*_urls.txt 2>/dev/null | \
    grep -Ei '\.js($|\?|\#)' | \
    sort -u > "$OUT/all_js.txt"

JS_TOTAL=$(wc -l < "$OUT/all_js.txt")
echo -e "${GREEN}[+] $JS_TOTAL JS files discovered${NC}\n"

# 6. Live JS only (using curl for reliability)
echo -e "${YELLOW}[6] Probing live JS files (this may take a while)...${NC}"
> "$OUT/js_live.txt"
cat "$OUT/all_js.txt" | head -100 | while read jsurl; do
    if timeout 5 curl -sf -o /dev/null "$jsurl" 2>/dev/null; then
        echo "$jsurl" >> "$OUT/js_live.txt"
        echo "  [live] $jsurl"
    fi
done

JS_LIVE=$(wc -l < "$OUT/js_live.txt" 2>/dev/null || echo "0")
echo -e "${GREEN}[+] $JS_LIVE live JS files${NC}\n"

# 7. Extract endpoints/URLs from JS file content
echo -e "${YELLOW}[7] Extracting endpoints from live JS...${NC}"
> "$OUT/js_endpoints.txt"
if [[ -s "$OUT/js_live.txt" ]]; then
    while IFS= read -r jsurl; do
        echo "  [extracting] $jsurl"
        # Download JS content and extract URLs using regex
        timeout 10 curl -sf "$jsurl" 2>/dev/null | \
            grep -oE '(https?://[^"'\''[:space:]<>()]+|/[a-zA-Z0-9/_.-]+)' | \
            grep -E '^(https?://|/)' | \
            sed "s|^/|https://$(echo $jsurl | cut -d'/' -f3)/|" >> "$OUT/js_endpoints.txt" 2>/dev/null || true
    done < "$OUT/js_live.txt"
    sort -u "$OUT/js_endpoints.txt" -o "$OUT/js_endpoints.txt" 2>/dev/null || true
else
    echo "  [skipped] No live JS files to process"
fi

ENDPOINTS=$(wc -l < "$OUT/js_endpoints.txt" 2>/dev/null || echo "0")
echo -e "${GREEN}[+] $ENDPOINTS endpoints from JS${NC}\n"

# 8. Secrets (scan JS content for sensitive data)
echo -e "${YELLOW}[8] Secret hunting...${NC}"
> "$OUT/secrets.txt"
if [[ -s "$OUT/js_live.txt" ]]; then
    while IFS= read -r jsurl; do
        echo "  [scanning] $jsurl"
        timeout 10 curl -sf "$jsurl" 2>/dev/null | grep -iE "(key|secret|token|pass|aws_|bearer|auth|api)" >> "$OUT/secrets.txt" 2>/dev/null || true
    done < "$OUT/js_live.txt"
    sort -u "$OUT/secrets.txt" -o "$OUT/secrets.txt" 2>/dev/null || true
else
    echo "  [skipped] No live JS files to scan"
fi

SECRETS=$(wc -l < "$OUT/secrets.txt" 2>/dev/null || echo "0")
echo -e "${GREEN}[+] $SECRETS potential secrets${NC}\n"

# Final report
echo -e "\n${GREEN}══════════════════ BULLETPROOF RESULTS ══════════════════${NC}"
echo -e "${GREEN}Alive domains     : $ALIVE${NC}"
echo -e "${GREEN}Total JS files    : $JS_TOTAL${NC}"
echo -e "${GREEN}Live JS files     : $JS_LIVE${NC}"
echo -e "${GREEN}JS endpoints      : $ENDPOINTS${NC}"
echo -e "${GREEN}Potential secrets : $SECRETS${NC}"
echo -e "${YELLOW}All in → $OUT/${NC}"

exit 0