#!/bin/bash
# ╔══════════════════════════════════════════════════════════╗
# ║           BULLETPROOF JS CRAWLER (Dec 2025)              ║
# ║                                                          ║
# ║               ~5–15 min on 5k+ domains                   ║
# ╚══════════════════════════════════════════════════════════╝
set -euo pipefail

RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'; NC='\033[0m'
FILE="${1:-}"
BASE_PATH="${2:-.}"
TARGET=$(basename "$FILE" | sed 's/\.subdomains\.txt$//' | sed 's/\.txt$//')
OUT="$BASE_PATH/${TARGET}_JS_$(date +%H%M%S)"
THREADS=100
PARALLEL=20  # Number of parallel jobs for gau-tool/wayback

[[ -z "$FILE" || ! -f "$FILE" ]] && { echo -e "${RED}Usage: $0 subdomains.txt [base_output_path]${NC}"; exit 1; }

mkdir -p "$OUT"
echo -e "${GREEN}[+] Bulletproof JS crawl starting → $OUT${NC}\n"

# 1. Get alive 200-OK domains (httpx – bulletproof)
echo -e "${YELLOW}[1] Probing 200-OK domains...${NC}"
cat "$FILE" | sed 's/^/https:\/\//' | \
    httpx -threads "$THREADS" -mc 200 -timeout 7 -retries 0 -fr -o "$OUT/alive.txt"

ALIVE=$(wc -l < "$OUT/alive.txt")
echo -e "${GREEN}[+] $ALIVE alive domains${NC}\n"

# 2. Archive URLs with waybackurls (skipping gau due to timeout issues)
echo -e "${YELLOW}[2] Fetching wayback archive URLs...${NC}"
> "$OUT/archive_urls.txt"
while IFS= read -r domain; do
    echo "  [wayback] $domain"
    timeout 30 waybackurls "$domain" 2>/dev/null >> "$OUT/archive_urls.txt" || echo "  [timeout] $domain"
done < "$OUT/alive.txt"
sort -u "$OUT/archive_urls.txt" -o "$OUT/archive_urls.txt"

echo -e "${GREEN}[+] Archive URLs: $(wc -l < "$OUT/archive_urls.txt")${NC}\n"

# 3. Katana (correct 2025 flags - removed headless mode to avoid timeouts)
echo -e "${YELLOW}[3] Katana crawling...${NC}"
katana -list "$OUT/alive.txt" \
    -d 3 \
    -c "$THREADS" \
    -jc \
    -xhr \
    -timeout 10 \
    -ef png,jpg,jpeg,gif,css,woff,woff2,svg,ico,pdf \
    -o "$OUT/katana_urls.txt" 2>/dev/null
echo -e "${GREEN}[+] Katana URLs: $(wc -l < "$OUT/katana_urls.txt")${NC}"

# 4. Hakrawler (FIXED: no -plain, use stdin pipe)
echo -e "${YELLOW}[4] Hakrawler crawling...${NC}"
cat "$OUT/alive.txt" | while read domain; do
    echo "  [crawling] $domain"
    echo "$domain"
done | hakrawler -d 3 -t 80 -subs -u 2>/dev/null > "$OUT/hakrawler_urls.txt"
echo -e "${GREEN}[+] Hakrawler URLs: $(wc -l < "$OUT/hakrawler_urls.txt")${NC}"

# 5. Extract JS URLs (regex fixed for extensions/queries)
echo -e "${YELLOW}[5] Extracting JS files...${NC}"
cat "$OUT"/*_urls.txt 2>/dev/null | \
    grep -Ei '\.js($|\?|\#)' | \
    sort -u > "$OUT/all_js.txt"

JS_TOTAL=$(wc -l < "$OUT/all_js.txt")
echo -e "${GREEN}[+] $JS_TOTAL JS files discovered${NC}\n"

# 6. Live JS only (httpx with error handling)
echo -e "${YELLOW}[6] Probing live JS files...${NC}"
httpx -l "$OUT/all_js.txt" -threads 200 -mc 200 -timeout 8 -o "$OUT/js_live.txt" 2>/dev/null || true

JS_LIVE=$(wc -l < "$OUT/js_live.txt" 2>/dev/null || echo "0")
echo -e "${GREEN}[+] $JS_LIVE live JS files${NC}\n"

# 7. Endpoints from live JS (FIXED: pipe URLs through hakrawler)
echo -e "${YELLOW}[7] Extracting endpoints from live JS...${NC}"
cat "$OUT/js_live.txt" | while read jsurl; do
    echo "  [extracting] $jsurl"
    echo "$jsurl"
done | hakrawler -d 1 -t 20 -u 2>/dev/null | grep "^http" > "$OUT/js_endpoints.txt" || true

sort -u "$OUT/js_endpoints.txt" -o "$OUT/js_endpoints.txt"
ENDPOINTS=$(wc -l < "$OUT/js_endpoints.txt")
echo -e "${GREEN}[+] $ENDPOINTS endpoints from JS${NC}\n"

# 8. Secrets (getJS is fast & reliable)
echo -e "${YELLOW}[8] Secret hunting...${NC}"
cat "$OUT/js_live.txt" | while read jsurl; do
    echo "  [scanning] $jsurl"
    echo "$jsurl"
done | getJS -complete 2>/dev/null | \
    grep -iE "(key|secret|token|pass|aws_|bearer|auth)" > "$OUT/secrets.txt" || true

SECRETS=$(wc -l < "$OUT/secrets.txt")
echo -e "${GREEN}[+] $SECRETS potential secrets${NC}\n"

# Final report
echo -e "\n${GREEN}══════════════════ BULLETPROOF RESULTS ══════════════════${NC}"
echo -e "${GREEN}Alive domains     : $ALIVE${NC}"
echo -e "${GREEN}Total JS files    : $JS_TOTAL${NC}"
echo -e "${GREEN}Live JS files     : $JS_LIVE${NC}"
echo -e "${GREEN}JS endpoints      : $ENDPOINTS${NC}"
echo -e "${GREEN}Potential secrets : $SECRETS${NC}"
echo -e "${YELLOW}All in → $OUT/${NC}"
echo -e "${YELLOW}Next: cat $OUT/js_endpoints.txt | gf redirect | httpx -mc 302${NC}"

exit 0